{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-28T16:17:06.131598Z",
     "start_time": "2026-01-28T16:17:02.176276Z"
    }
   },
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.gpt2 import GPT2, GPT2Config  # <- ton .py\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # si dispo, accélère souvent\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# --- Modèle (bon compromis pour une carte grand public)\n",
    "config = GPT2Config(\n",
    "    block_size=512,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    n_embd=768,\n",
    "    dropout=0.1,\n",
    "    vocab_size=50257\n",
    ")\n",
    "model = GPT2(config).to(device)\n",
    "\n",
    "# --- Hyperparams (à ajuster selon VRAM)\n",
    "batch_size = 8  # micro-batch\n",
    "grad_accum = 4  # batch effectif = 32\n",
    "max_steps = 200_000\n",
    "warmup_steps = 2_000\n",
    "max_lr = 3e-4\n",
    "weight_decay = 0.1\n",
    "grad_clip = 1.0\n",
    "\n",
    "use_amp = True\n",
    "scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), weight_decay=weight_decay)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\libry\\AppData\\Local\\Temp\\ipykernel_23140\\1708106867.py:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:17:16.128411Z",
     "start_time": "2026-01-28T16:17:11.365406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "\n",
    "tokenized = ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "def build_stream(split):\n",
    "    ids = []\n",
    "    for ex in split:\n",
    "        ids.extend(ex[\"input_ids\"])\n",
    "        ids.append(tokenizer.eos_token_id)\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_stream = build_stream(tokenized[\"train\"])\n",
    "val_stream = build_stream(tokenized[\"validation\"])\n",
    "\n",
    "\n",
    "class StreamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, stream_ids, block_size):\n",
    "        self.data = stream_ids\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - 1) // self.block_size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = i * self.block_size\n",
    "        x = self.data[s:s + self.block_size]\n",
    "        y = self.data[s + 1:s + 1 + self.block_size]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_ds = StreamDataset(train_stream, config.block_size)\n",
    "val_ds = StreamDataset(val_stream, config.block_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n"
   ],
   "id": "7cc14b3c7e92a083",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:17:18.825483Z",
     "start_time": "2026-01-28T16:17:18.820383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_lr(step, warmup_steps, max_steps, max_lr):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * step / warmup_steps\n",
    "    progress = (step - warmup_steps) / max(1, (max_steps - warmup_steps))\n",
    "    return 0.5 * max_lr * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=50):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n"
   ],
   "id": "8328ad93c07c2fdf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:17:20.810163Z",
     "start_time": "2026-01-28T16:17:20.804050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_dir = r\"C:\\workspace\\GPT2\\models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(step, model, optimizer, config, extra=None):\n",
    "    path = os.path.join(save_dir, f\"ckpt_step{step}.pt\")\n",
    "    payload = {\n",
    "        \"step\": step,\n",
    "        \"config\": config.__dict__,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "    }\n",
    "    if extra is not None:\n",
    "        payload[\"extra\"] = extra\n",
    "    torch.save(payload, path)\n",
    "    # aussi un \"last\" pratique\n",
    "    torch.save(payload, os.path.join(save_dir, \"ckpt_last.pt\"))\n",
    "    return path\n"
   ],
   "id": "33689d0bbe2490a5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T16:22:14.009983Z",
     "start_time": "2026-01-28T16:17:22.905229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_every = 50\n",
    "eval_every = 1000\n",
    "save_every = 10_000\n",
    "\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(10_000):  # on break via max_steps\n",
    "    for x, y in train_loader:\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "\n",
    "        lr = get_lr(step, warmup_steps, max_steps, max_lr)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=use_amp):\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / grad_accum  # important\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % grad_accum == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            dt = time.time() - t0\n",
    "            print(f\"step {step:7d} | loss {(loss.item() * grad_accum):.4f} | lr {lr:.2e} | {dt:.1f}s\")\n",
    "            t0 = time.time()\n",
    "\n",
    "        if step % eval_every == 0 and step > 0:\n",
    "            val_loss = evaluate(model, val_loader, max_batches=50)\n",
    "            print(f\"  eval | val_loss {val_loss:.4f} | ppl {math.exp(val_loss):.2f}\")\n",
    "\n",
    "        if step % save_every == 0 and step > 0:\n",
    "            path = save_checkpoint(step, model, optimizer, config)\n",
    "            print(f\"  ✅ saved: {path}\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    if step >= max_steps:\n",
    "        break\n"
   ],
   "id": "68256653f763f92",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\libry\\AppData\\Local\\Temp\\ipykernel_23140\\2015531958.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step       0 | loss 10.9493 | lr 0.00e+00 | 0.3s\n",
      "step      50 | loss 10.3451 | lr 7.50e-06 | 7.6s\n",
      "step     100 | loss 9.7374 | lr 1.50e-05 | 7.2s\n",
      "step     150 | loss 9.5159 | lr 2.25e-05 | 7.3s\n",
      "step     200 | loss 9.1871 | lr 3.00e-05 | 7.6s\n",
      "step     250 | loss 8.7781 | lr 3.75e-05 | 7.3s\n",
      "step     300 | loss 8.0904 | lr 4.50e-05 | 7.4s\n",
      "step     350 | loss 7.8454 | lr 5.25e-05 | 7.6s\n",
      "step     400 | loss 7.4752 | lr 6.00e-05 | 7.3s\n",
      "step     450 | loss 7.1099 | lr 6.75e-05 | 7.1s\n",
      "step     500 | loss 6.8796 | lr 7.50e-05 | 7.2s\n",
      "step     550 | loss 7.0984 | lr 8.25e-05 | 7.3s\n",
      "step     600 | loss 6.9429 | lr 9.00e-05 | 7.3s\n",
      "step     650 | loss 6.8454 | lr 9.75e-05 | 7.3s\n",
      "step     700 | loss 6.5779 | lr 1.05e-04 | 7.3s\n",
      "step     750 | loss 6.5341 | lr 1.12e-04 | 7.3s\n",
      "step     800 | loss 6.4674 | lr 1.20e-04 | 7.2s\n",
      "step     850 | loss 6.3940 | lr 1.28e-04 | 7.3s\n",
      "step     900 | loss 6.5261 | lr 1.35e-04 | 7.4s\n",
      "step     950 | loss 6.3990 | lr 1.42e-04 | 7.2s\n",
      "step    1000 | loss 6.4525 | lr 1.50e-04 | 7.2s\n",
      "  eval | val_loss 6.3253 | ppl 558.54\n",
      "step    1050 | loss 6.2200 | lr 1.57e-04 | 10.2s\n",
      "step    1100 | loss 6.2859 | lr 1.65e-04 | 7.1s\n",
      "step    1150 | loss 6.2185 | lr 1.72e-04 | 7.2s\n",
      "step    1200 | loss 6.0156 | lr 1.80e-04 | 7.2s\n",
      "step    1250 | loss 6.0295 | lr 1.87e-04 | 7.2s\n",
      "step    1300 | loss 6.0897 | lr 1.95e-04 | 7.3s\n",
      "step    1350 | loss 5.7530 | lr 2.02e-04 | 7.2s\n",
      "step    1400 | loss 5.9013 | lr 2.10e-04 | 7.2s\n",
      "step    1450 | loss 6.1224 | lr 2.17e-04 | 7.2s\n",
      "step    1500 | loss 6.0343 | lr 2.25e-04 | 7.3s\n",
      "step    1550 | loss 5.9049 | lr 2.32e-04 | 7.2s\n",
      "step    1600 | loss 5.9205 | lr 2.40e-04 | 7.2s\n",
      "step    1650 | loss 5.9762 | lr 2.47e-04 | 7.2s\n",
      "step    1700 | loss 5.5771 | lr 2.55e-04 | 7.3s\n",
      "step    1750 | loss 5.8295 | lr 2.62e-04 | 7.2s\n",
      "step    1800 | loss 5.6323 | lr 2.70e-04 | 7.3s\n",
      "step    1850 | loss 5.5612 | lr 2.77e-04 | 7.2s\n",
      "step    1900 | loss 5.5331 | lr 2.85e-04 | 7.3s\n",
      "step    1950 | loss 5.3835 | lr 2.93e-04 | 7.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     22\u001B[39m     _, loss = model(x, y)\n\u001B[32m     23\u001B[39m     loss = loss / grad_accum  \u001B[38;5;66;03m# important\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m \u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (step + \u001B[32m1\u001B[39m) % grad_accum == \u001B[32m0\u001B[39m:\n\u001B[32m     28\u001B[39m     scaler.unscale_(optimizer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\workspace\\GPT2\\.venv\\Lib\\site-packages\\torch\\_tensor.py:630\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    620\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    622\u001B[39m         Tensor.backward,\n\u001B[32m    623\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    628\u001B[39m         inputs=inputs,\n\u001B[32m    629\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\workspace\\GPT2\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    359\u001B[39m     retain_graph = create_graph\n\u001B[32m    361\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    362\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    363\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m364\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    365\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    366\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    367\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    369\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\workspace\\GPT2\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    863\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    864\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m865\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    866\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    867\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    869\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "87138c27133ab5ee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
