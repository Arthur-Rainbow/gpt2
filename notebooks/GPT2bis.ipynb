{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-29T10:09:43.414134Z",
     "start_time": "2026-01-29T10:09:36.696067Z"
    }
   },
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.gpt2 import GPT2, GPT2Config, set_seed, StreamDataset, get_lr\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # si dispo, accélère souvent\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "config = GPT2Config(\n",
    "    block_size=512,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    n_embd=768,\n",
    "    dropout=0.1,\n",
    "    vocab_size=50257\n",
    ")\n",
    "\n",
    "model = GPT2(config).to(device)\n",
    "\n",
    "# --- Hyperparams\n",
    "batch_size = 8  # micro-batch\n",
    "grad_accum = 4  # batch effectif = 32\n",
    "max_steps = 20_000\n",
    "warmup_steps = 2_000\n",
    "max_lr = 3e-4\n",
    "weight_decay = 0.1\n",
    "grad_clip = 1.0\n",
    "\n",
    "use_amp = True\n",
    "scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), weight_decay=weight_decay)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\libry\\AppData\\Local\\Temp\\ipykernel_27916\\1464794925.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:09:48.386055Z",
     "start_time": "2026-01-29T10:09:43.422289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "\n",
    "tokenized = ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "def build_stream(split):\n",
    "    ids = []\n",
    "    for ex in split:\n",
    "        ids.extend(ex[\"input_ids\"])\n",
    "        ids.append(tokenizer.eos_token_id)\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_stream = build_stream(tokenized[\"train\"])\n",
    "val_stream = build_stream(tokenized[\"validation\"])\n",
    "\n",
    "train_ds = StreamDataset(train_stream, config.block_size)\n",
    "val_ds = StreamDataset(val_stream, config.block_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n"
   ],
   "id": "7cc14b3c7e92a083",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:09:48.393369Z",
     "start_time": "2026-01-29T10:09:48.388068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=50):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n"
   ],
   "id": "8328ad93c07c2fdf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T10:09:48.400575Z",
     "start_time": "2026-01-29T10:09:48.395210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_dir = r\"C:\\workspace\\GPT2\\models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(step, model, optimizer, config, extra=None):\n",
    "    path = os.path.join(save_dir, f\"ckpt_step{step}.pt\")\n",
    "    payload = {\n",
    "        \"step\": step,\n",
    "        \"config\": config.__dict__,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "    }\n",
    "    if extra is not None:\n",
    "        payload[\"extra\"] = extra\n",
    "    torch.save(payload, path)\n",
    "    # aussi un \"last\" pratique\n",
    "    torch.save(payload, os.path.join(save_dir, \"ckpt_last.pt\"))\n",
    "    return path"
   ],
   "id": "33689d0bbe2490a5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T11:00:42.240642Z",
     "start_time": "2026-01-29T10:09:48.400575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_every = 50\n",
    "eval_every = 500\n",
    "save_every = 1_000\n",
    "\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(10_000):  # on break via max_steps\n",
    "    for x, y in train_loader:\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "\n",
    "        lr = get_lr(step, warmup_steps, max_steps, max_lr)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=use_amp):\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / grad_accum  # important\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % grad_accum == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            dt = time.time() - t0\n",
    "            print(f\"step {step:7d} | loss {(loss.item() * grad_accum):.4f} | lr {lr:.2e} | {dt:.1f}s\")\n",
    "            t0 = time.time()\n",
    "\n",
    "        if step % eval_every == 0 and step > 0:\n",
    "            val_loss = evaluate(model, val_loader, max_batches=50)\n",
    "            print(f\"  eval | val_loss {val_loss:.4f} | ppl {math.exp(val_loss):.2f}\")\n",
    "\n",
    "        if step % save_every == 0 and step > 0:\n",
    "            path = save_checkpoint(step, model, optimizer, config)\n",
    "            print(f\"  ✅ saved: {path}\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    if step >= max_steps:\n",
    "        break"
   ],
   "id": "68256653f763f92",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\libry\\AppData\\Local\\Temp\\ipykernel_27916\\1287743397.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step       0 | loss 10.9829 | lr 0.00e+00 | 0.5s\n",
      "step      50 | loss 10.3415 | lr 7.50e-06 | 7.3s\n",
      "step     150 | loss 9.4357 | lr 2.25e-05 | 7.2s\n",
      "step     200 | loss 9.1855 | lr 3.00e-05 | 7.2s\n",
      "step     250 | loss 8.6493 | lr 3.75e-05 | 7.1s\n",
      "step     300 | loss 8.0987 | lr 4.50e-05 | 7.1s\n",
      "step     350 | loss 7.7415 | lr 5.25e-05 | 7.1s\n",
      "step     400 | loss 7.2956 | lr 6.00e-05 | 7.1s\n",
      "step     450 | loss 7.0578 | lr 6.75e-05 | 7.1s\n",
      "step     500 | loss 7.0417 | lr 7.50e-05 | 7.2s\n",
      "  eval | val_loss 6.9613 | ppl 1054.97\n",
      "step     550 | loss 6.8347 | lr 8.25e-05 | 10.3s\n",
      "step     600 | loss 6.7800 | lr 9.00e-05 | 7.1s\n",
      "step     650 | loss 6.8145 | lr 9.75e-05 | 7.1s\n",
      "step     700 | loss 6.7635 | lr 1.05e-04 | 7.1s\n",
      "step     750 | loss 6.5073 | lr 1.12e-04 | 7.2s\n",
      "step     800 | loss 6.5297 | lr 1.20e-04 | 7.1s\n",
      "step     850 | loss 6.5149 | lr 1.28e-04 | 7.1s\n",
      "step     900 | loss 6.4721 | lr 1.35e-04 | 7.1s\n",
      "step     950 | loss 6.4713 | lr 1.42e-04 | 7.1s\n",
      "step    1000 | loss 6.3468 | lr 1.50e-04 | 7.2s\n",
      "  eval | val_loss 6.3310 | ppl 561.73\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step1000.pt\n",
      "step    1050 | loss 6.4437 | lr 1.57e-04 | 26.4s\n",
      "step    1100 | loss 6.2274 | lr 1.65e-04 | 7.1s\n",
      "step    1150 | loss 6.0134 | lr 1.72e-04 | 7.1s\n",
      "step    1200 | loss 6.0864 | lr 1.80e-04 | 7.1s\n",
      "step    1250 | loss 6.1073 | lr 1.87e-04 | 7.1s\n",
      "step    1300 | loss 6.1574 | lr 1.95e-04 | 7.1s\n",
      "step    1350 | loss 5.9346 | lr 2.02e-04 | 7.1s\n",
      "step    1400 | loss 5.9012 | lr 2.10e-04 | 7.1s\n",
      "step    1450 | loss 6.0417 | lr 2.17e-04 | 7.0s\n",
      "step    1500 | loss 5.6386 | lr 2.25e-04 | 7.1s\n",
      "  eval | val_loss 5.9644 | ppl 389.30\n",
      "step    1550 | loss 5.7707 | lr 2.32e-04 | 10.1s\n",
      "step    1600 | loss 5.7382 | lr 2.40e-04 | 7.1s\n",
      "step    1650 | loss 5.9653 | lr 2.47e-04 | 7.1s\n",
      "step    1700 | loss 5.7077 | lr 2.55e-04 | 7.1s\n",
      "step    1750 | loss 5.8003 | lr 2.62e-04 | 7.3s\n",
      "step    1800 | loss 5.4447 | lr 2.70e-04 | 7.1s\n",
      "step    1850 | loss 5.5684 | lr 2.77e-04 | 7.1s\n",
      "step    1900 | loss 5.5477 | lr 2.85e-04 | 7.1s\n",
      "step    1950 | loss 5.4603 | lr 2.93e-04 | 7.1s\n",
      "step    2000 | loss 5.6695 | lr 3.00e-04 | 7.1s\n",
      "  eval | val_loss 5.7573 | ppl 316.50\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step2000.pt\n",
      "step    2050 | loss 5.4500 | lr 3.00e-04 | 12.6s\n",
      "step    2100 | loss 5.3375 | lr 3.00e-04 | 7.1s\n",
      "step    2150 | loss 5.2277 | lr 3.00e-04 | 7.0s\n",
      "step    2200 | loss 5.4040 | lr 3.00e-04 | 7.1s\n",
      "step    2250 | loss 5.5644 | lr 3.00e-04 | 7.0s\n",
      "step    2300 | loss 5.0818 | lr 3.00e-04 | 7.1s\n",
      "step    2350 | loss 5.2347 | lr 3.00e-04 | 7.1s\n",
      "step    2400 | loss 4.9421 | lr 3.00e-04 | 7.1s\n",
      "step    2450 | loss 5.0447 | lr 3.00e-04 | 7.0s\n",
      "step    2500 | loss 5.1068 | lr 2.99e-04 | 7.1s\n",
      "  eval | val_loss 5.5693 | ppl 262.25\n",
      "step    2550 | loss 5.2486 | lr 2.99e-04 | 10.1s\n",
      "step    2600 | loss 5.0056 | lr 2.99e-04 | 7.1s\n",
      "step    2650 | loss 5.0963 | lr 2.99e-04 | 7.0s\n",
      "step    2700 | loss 5.2096 | lr 2.99e-04 | 7.1s\n",
      "step    2750 | loss 5.2755 | lr 2.99e-04 | 7.6s\n",
      "step    2800 | loss 4.8497 | lr 2.99e-04 | 7.1s\n",
      "step    2850 | loss 5.2004 | lr 2.98e-04 | 7.1s\n",
      "step    2900 | loss 5.3280 | lr 2.98e-04 | 7.2s\n",
      "step    2950 | loss 5.1606 | lr 2.98e-04 | 7.1s\n",
      "step    3000 | loss 4.8424 | lr 2.98e-04 | 7.1s\n",
      "  eval | val_loss 5.4357 | ppl 229.45\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step3000.pt\n",
      "step    3050 | loss 4.7832 | lr 2.97e-04 | 12.4s\n",
      "step    3100 | loss 4.5611 | lr 2.97e-04 | 7.1s\n",
      "step    3150 | loss 5.1665 | lr 2.97e-04 | 7.2s\n",
      "step    3200 | loss 4.7009 | lr 2.97e-04 | 7.1s\n",
      "step    3250 | loss 4.9254 | lr 2.96e-04 | 7.1s\n",
      "step    3300 | loss 4.9461 | lr 2.96e-04 | 7.2s\n",
      "step    3350 | loss 4.6848 | lr 2.96e-04 | 7.1s\n",
      "step    3400 | loss 4.9824 | lr 2.96e-04 | 7.1s\n",
      "step    3450 | loss 4.7147 | lr 2.95e-04 | 7.1s\n",
      "step    3500 | loss 4.8420 | lr 2.95e-04 | 7.1s\n",
      "  eval | val_loss 5.3041 | ppl 201.15\n",
      "step    3550 | loss 4.8690 | lr 2.95e-04 | 10.1s\n",
      "step    3600 | loss 4.5994 | lr 2.94e-04 | 7.1s\n",
      "step    3650 | loss 4.5114 | lr 2.94e-04 | 7.1s\n",
      "step    3700 | loss 4.3859 | lr 2.93e-04 | 7.1s\n",
      "step    3750 | loss 4.6288 | lr 2.93e-04 | 7.1s\n",
      "step    3800 | loss 4.4095 | lr 2.93e-04 | 7.1s\n",
      "step    3850 | loss 4.5992 | lr 2.92e-04 | 7.1s\n",
      "step    3900 | loss 4.7704 | lr 2.92e-04 | 7.1s\n",
      "step    3950 | loss 4.6506 | lr 2.91e-04 | 7.1s\n",
      "step    4000 | loss 4.7182 | lr 2.91e-04 | 7.1s\n",
      "  eval | val_loss 5.2136 | ppl 183.76\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step4000.pt\n",
      "step    4050 | loss 4.5809 | lr 2.91e-04 | 12.1s\n",
      "step    4100 | loss 4.4207 | lr 2.90e-04 | 7.1s\n",
      "step    4150 | loss 4.1047 | lr 2.90e-04 | 7.1s\n",
      "step    4200 | loss 4.3325 | lr 2.89e-04 | 7.1s\n",
      "step    4250 | loss 4.4810 | lr 2.89e-04 | 7.1s\n",
      "step    4300 | loss 4.3135 | lr 2.88e-04 | 7.1s\n",
      "step    4350 | loss 4.2865 | lr 2.88e-04 | 7.1s\n",
      "step    4400 | loss 4.4982 | lr 2.87e-04 | 7.1s\n",
      "step    4450 | loss 4.6363 | lr 2.86e-04 | 7.1s\n",
      "step    4500 | loss 4.2958 | lr 2.86e-04 | 7.1s\n",
      "  eval | val_loss 5.1453 | ppl 171.63\n",
      "step    4550 | loss 4.4093 | lr 2.85e-04 | 10.1s\n",
      "step    4600 | loss 4.4798 | lr 2.85e-04 | 7.1s\n",
      "step    4650 | loss 4.5174 | lr 2.84e-04 | 7.1s\n",
      "step    4700 | loss 4.4181 | lr 2.84e-04 | 7.1s\n",
      "step    4750 | loss 4.0357 | lr 2.83e-04 | 7.1s\n",
      "step    4800 | loss 3.9310 | lr 2.82e-04 | 7.1s\n",
      "step    4850 | loss 3.9914 | lr 2.82e-04 | 7.1s\n",
      "step    4900 | loss 4.3115 | lr 2.81e-04 | 7.1s\n",
      "step    4950 | loss 4.1789 | lr 2.81e-04 | 7.1s\n",
      "step    5000 | loss 4.1550 | lr 2.80e-04 | 7.1s\n",
      "  eval | val_loss 5.1078 | ppl 165.31\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step5000.pt\n",
      "step    5050 | loss 4.2036 | lr 2.79e-04 | 12.2s\n",
      "step    5100 | loss 3.6551 | lr 2.79e-04 | 7.1s\n",
      "step    5150 | loss 4.2005 | lr 2.78e-04 | 7.1s\n",
      "step    5200 | loss 4.0151 | lr 2.77e-04 | 7.1s\n",
      "step    5250 | loss 4.1506 | lr 2.77e-04 | 7.1s\n",
      "step    5300 | loss 4.2996 | lr 2.76e-04 | 7.1s\n",
      "step    5350 | loss 3.7355 | lr 2.75e-04 | 7.1s\n",
      "step    5400 | loss 3.9501 | lr 2.74e-04 | 7.1s\n",
      "step    5450 | loss 3.5874 | lr 2.74e-04 | 7.1s\n",
      "step    5500 | loss 3.6493 | lr 2.73e-04 | 7.1s\n",
      "  eval | val_loss 5.0603 | ppl 157.64\n",
      "step    5550 | loss 3.5129 | lr 2.72e-04 | 10.1s\n",
      "step    5600 | loss 3.9453 | lr 2.71e-04 | 7.1s\n",
      "step    5650 | loss 4.1350 | lr 2.71e-04 | 7.1s\n",
      "step    5700 | loss 4.0880 | lr 2.70e-04 | 7.1s\n",
      "step    5750 | loss 3.9032 | lr 2.69e-04 | 7.1s\n",
      "step    5800 | loss 4.0312 | lr 2.68e-04 | 7.1s\n",
      "step    5850 | loss 3.8791 | lr 2.67e-04 | 7.1s\n",
      "step    5900 | loss 3.9827 | lr 2.67e-04 | 7.1s\n",
      "step    5950 | loss 3.7848 | lr 2.66e-04 | 7.1s\n",
      "step    6000 | loss 3.5697 | lr 2.65e-04 | 7.1s\n",
      "  eval | val_loss 5.0516 | ppl 156.28\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step6000.pt\n",
      "step    6050 | loss 3.6962 | lr 2.64e-04 | 12.1s\n",
      "step    6100 | loss 3.8929 | lr 2.63e-04 | 7.1s\n",
      "step    6150 | loss 4.0332 | lr 2.62e-04 | 7.1s\n",
      "step    6200 | loss 3.7559 | lr 2.61e-04 | 7.1s\n",
      "step    6250 | loss 3.8420 | lr 2.61e-04 | 7.1s\n",
      "step    6300 | loss 3.5397 | lr 2.60e-04 | 7.1s\n",
      "step    6350 | loss 3.8740 | lr 2.59e-04 | 7.1s\n",
      "step    6400 | loss 3.7073 | lr 2.58e-04 | 7.1s\n",
      "step    6450 | loss 3.6952 | lr 2.57e-04 | 7.1s\n",
      "step    6500 | loss 3.8268 | lr 2.56e-04 | 7.1s\n",
      "  eval | val_loss 5.0181 | ppl 151.12\n",
      "step    6550 | loss 3.3266 | lr 2.55e-04 | 10.1s\n",
      "step    6600 | loss 3.4141 | lr 2.54e-04 | 7.1s\n",
      "step    6650 | loss 3.1667 | lr 2.53e-04 | 7.1s\n",
      "step    6700 | loss 3.1507 | lr 2.52e-04 | 7.1s\n",
      "step    6750 | loss 3.3676 | lr 2.51e-04 | 7.1s\n",
      "step    6800 | loss 3.5629 | lr 2.50e-04 | 7.1s\n",
      "step    6850 | loss 3.4610 | lr 2.49e-04 | 7.1s\n",
      "step    6900 | loss 3.5272 | lr 2.48e-04 | 7.1s\n",
      "step    6950 | loss 3.2195 | lr 2.47e-04 | 7.1s\n",
      "step    7000 | loss 3.3641 | lr 2.46e-04 | 7.1s\n",
      "  eval | val_loss 5.0407 | ppl 154.58\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step7000.pt\n",
      "step    7050 | loss 3.3990 | lr 2.45e-04 | 12.1s\n",
      "step    7100 | loss 3.6036 | lr 2.44e-04 | 7.1s\n",
      "step    7150 | loss 3.0773 | lr 2.43e-04 | 7.1s\n",
      "step    7200 | loss 3.1381 | lr 2.42e-04 | 7.1s\n",
      "step    7250 | loss 3.2868 | lr 2.41e-04 | 7.1s\n",
      "step    7300 | loss 3.1579 | lr 2.40e-04 | 7.1s\n",
      "step    7350 | loss 3.4207 | lr 2.39e-04 | 7.1s\n",
      "step    7400 | loss 3.3803 | lr 2.38e-04 | 7.1s\n",
      "step    7450 | loss 3.2778 | lr 2.37e-04 | 7.1s\n",
      "step    7500 | loss 3.2376 | lr 2.36e-04 | 7.1s\n",
      "  eval | val_loss 5.0829 | ppl 161.23\n",
      "step    7550 | loss 3.4306 | lr 2.35e-04 | 10.1s\n",
      "step    7600 | loss 3.3382 | lr 2.34e-04 | 7.1s\n",
      "step    7650 | loss 3.5072 | lr 2.33e-04 | 7.1s\n",
      "step    7700 | loss 3.0674 | lr 2.32e-04 | 7.1s\n",
      "step    7750 | loss 3.1954 | lr 2.31e-04 | 7.1s\n",
      "step    7800 | loss 3.0158 | lr 2.29e-04 | 7.1s\n",
      "step    7850 | loss 3.0338 | lr 2.28e-04 | 7.1s\n",
      "step    7900 | loss 3.2293 | lr 2.27e-04 | 7.1s\n",
      "step    7950 | loss 2.9441 | lr 2.26e-04 | 7.1s\n",
      "step    8000 | loss 3.2757 | lr 2.25e-04 | 7.1s\n",
      "  eval | val_loss 5.1210 | ppl 167.50\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step8000.pt\n",
      "step    8050 | loss 3.0357 | lr 2.24e-04 | 12.1s\n",
      "step    8100 | loss 3.3753 | lr 2.23e-04 | 7.1s\n",
      "step    8150 | loss 3.0921 | lr 2.22e-04 | 7.1s\n",
      "step    8200 | loss 3.1429 | lr 2.20e-04 | 7.1s\n",
      "step    8250 | loss 3.1579 | lr 2.19e-04 | 7.1s\n",
      "step    8300 | loss 2.8896 | lr 2.18e-04 | 7.1s\n",
      "step    8350 | loss 2.6130 | lr 2.17e-04 | 7.2s\n",
      "step    8400 | loss 2.9062 | lr 2.16e-04 | 7.2s\n",
      "step    8450 | loss 2.8898 | lr 2.15e-04 | 7.1s\n",
      "step    8500 | loss 2.8392 | lr 2.13e-04 | 7.2s\n",
      "  eval | val_loss 5.1751 | ppl 176.81\n",
      "step    8550 | loss 3.0819 | lr 2.12e-04 | 10.2s\n",
      "step    8600 | loss 2.8562 | lr 2.11e-04 | 7.1s\n",
      "step    8650 | loss 2.9201 | lr 2.10e-04 | 7.1s\n",
      "step    8700 | loss 3.0320 | lr 2.09e-04 | 7.1s\n",
      "step    8750 | loss 2.9253 | lr 2.07e-04 | 7.1s\n",
      "step    8800 | loss 2.9328 | lr 2.06e-04 | 7.1s\n",
      "step    8850 | loss 2.8476 | lr 2.05e-04 | 7.2s\n",
      "step    8900 | loss 2.7745 | lr 2.04e-04 | 7.2s\n",
      "step    8950 | loss 2.6426 | lr 2.03e-04 | 7.2s\n",
      "step    9000 | loss 2.8074 | lr 2.01e-04 | 7.2s\n",
      "  eval | val_loss 5.2385 | ppl 188.39\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step9000.pt\n",
      "step    9050 | loss 2.6320 | lr 2.00e-04 | 12.2s\n",
      "step    9100 | loss 2.7277 | lr 1.99e-04 | 7.2s\n",
      "step    9150 | loss 2.6652 | lr 1.98e-04 | 7.2s\n",
      "step    9200 | loss 2.5814 | lr 1.96e-04 | 7.1s\n",
      "step    9250 | loss 2.7236 | lr 1.95e-04 | 7.2s\n",
      "step    9300 | loss 2.8565 | lr 1.94e-04 | 7.2s\n",
      "step    9350 | loss 2.5144 | lr 1.93e-04 | 7.1s\n",
      "step    9400 | loss 2.8827 | lr 1.91e-04 | 7.2s\n",
      "step    9450 | loss 2.7290 | lr 1.90e-04 | 7.2s\n",
      "step    9500 | loss 2.4770 | lr 1.89e-04 | 7.2s\n",
      "  eval | val_loss 5.2800 | ppl 196.37\n",
      "step    9550 | loss 2.3641 | lr 1.88e-04 | 10.3s\n",
      "step    9600 | loss 2.4177 | lr 1.86e-04 | 7.3s\n",
      "step    9650 | loss 2.7284 | lr 1.85e-04 | 7.2s\n",
      "step    9700 | loss 2.5048 | lr 1.84e-04 | 7.1s\n",
      "step    9750 | loss 2.5914 | lr 1.82e-04 | 7.1s\n",
      "step    9800 | loss 2.4885 | lr 1.81e-04 | 7.1s\n",
      "step    9850 | loss 2.5660 | lr 1.80e-04 | 7.2s\n",
      "step    9900 | loss 2.4885 | lr 1.79e-04 | 7.2s\n",
      "step    9950 | loss 2.6442 | lr 1.77e-04 | 7.2s\n",
      "step   10000 | loss 2.5971 | lr 1.76e-04 | 7.1s\n",
      "  eval | val_loss 5.3183 | ppl 204.05\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step10000.pt\n",
      "step   10050 | loss 2.4954 | lr 1.75e-04 | 12.4s\n",
      "step   10100 | loss 2.2183 | lr 1.73e-04 | 7.1s\n",
      "step   10150 | loss 2.2902 | lr 1.72e-04 | 7.1s\n",
      "step   10200 | loss 2.4200 | lr 1.71e-04 | 7.1s\n",
      "step   10250 | loss 2.2640 | lr 1.70e-04 | 7.1s\n",
      "step   10300 | loss 2.5659 | lr 1.68e-04 | 7.1s\n",
      "step   10350 | loss 2.3943 | lr 1.67e-04 | 7.2s\n",
      "step   10400 | loss 2.3734 | lr 1.66e-04 | 7.1s\n",
      "step   10450 | loss 2.3918 | lr 1.64e-04 | 7.1s\n",
      "step   10500 | loss 2.1234 | lr 1.63e-04 | 7.1s\n",
      "  eval | val_loss 5.4171 | ppl 225.22\n",
      "step   10550 | loss 2.1719 | lr 1.62e-04 | 10.5s\n",
      "step   10600 | loss 2.3180 | lr 1.60e-04 | 7.2s\n",
      "step   10650 | loss 2.4072 | lr 1.59e-04 | 7.2s\n",
      "step   10700 | loss 2.1513 | lr 1.58e-04 | 7.1s\n",
      "step   10750 | loss 2.1144 | lr 1.57e-04 | 7.1s\n",
      "step   10800 | loss 2.0620 | lr 1.55e-04 | 7.1s\n",
      "step   10850 | loss 2.1565 | lr 1.54e-04 | 7.2s\n",
      "step   10900 | loss 2.3140 | lr 1.53e-04 | 7.1s\n",
      "step   10950 | loss 2.1503 | lr 1.51e-04 | 7.1s\n",
      "step   11000 | loss 2.2913 | lr 1.50e-04 | 7.1s\n",
      "  eval | val_loss 5.4915 | ppl 242.63\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step11000.pt\n",
      "step   11050 | loss 2.1781 | lr 1.49e-04 | 12.4s\n",
      "step   11100 | loss 2.1887 | lr 1.47e-04 | 7.1s\n",
      "step   11150 | loss 2.1935 | lr 1.46e-04 | 7.1s\n",
      "step   11200 | loss 2.1919 | lr 1.45e-04 | 7.1s\n",
      "step   11250 | loss 1.8459 | lr 1.43e-04 | 7.1s\n",
      "step   11300 | loss 1.8243 | lr 1.42e-04 | 7.2s\n",
      "step   11350 | loss 2.1200 | lr 1.41e-04 | 7.1s\n",
      "step   11400 | loss 1.9709 | lr 1.40e-04 | 7.2s\n",
      "step   11450 | loss 2.1720 | lr 1.38e-04 | 7.3s\n",
      "step   11500 | loss 2.0445 | lr 1.37e-04 | 7.2s\n",
      "  eval | val_loss 5.5731 | ppl 263.26\n",
      "step   11550 | loss 2.0180 | lr 1.36e-04 | 10.2s\n",
      "step   11600 | loss 2.0832 | lr 1.34e-04 | 7.1s\n",
      "step   11650 | loss 2.0392 | lr 1.33e-04 | 7.2s\n",
      "step   11700 | loss 1.8273 | lr 1.32e-04 | 7.3s\n",
      "step   11750 | loss 2.0071 | lr 1.30e-04 | 7.3s\n",
      "step   11800 | loss 2.1815 | lr 1.29e-04 | 7.2s\n",
      "step   11850 | loss 1.9323 | lr 1.28e-04 | 7.1s\n",
      "step   11900 | loss 2.0087 | lr 1.27e-04 | 7.1s\n",
      "step   11950 | loss 1.9789 | lr 1.25e-04 | 7.2s\n",
      "step   12000 | loss 1.9180 | lr 1.24e-04 | 7.1s\n",
      "  eval | val_loss 5.6577 | ppl 286.48\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step12000.pt\n",
      "step   12050 | loss 1.8797 | lr 1.23e-04 | 12.1s\n",
      "step   12100 | loss 1.9745 | lr 1.21e-04 | 7.1s\n",
      "step   12150 | loss 1.9097 | lr 1.20e-04 | 7.1s\n",
      "step   12200 | loss 1.9573 | lr 1.19e-04 | 7.1s\n",
      "step   12250 | loss 1.7423 | lr 1.18e-04 | 7.1s\n",
      "step   12300 | loss 1.9891 | lr 1.16e-04 | 7.3s\n",
      "step   12350 | loss 1.9107 | lr 1.15e-04 | 7.1s\n",
      "step   12400 | loss 1.9382 | lr 1.14e-04 | 7.1s\n",
      "step   12450 | loss 1.7550 | lr 1.12e-04 | 7.0s\n",
      "step   12500 | loss 1.7680 | lr 1.11e-04 | 7.1s\n",
      "  eval | val_loss 5.7113 | ppl 302.27\n",
      "step   12550 | loss 1.7328 | lr 1.10e-04 | 10.1s\n",
      "step   12600 | loss 1.8710 | lr 1.09e-04 | 7.1s\n",
      "step   12650 | loss 1.8251 | lr 1.07e-04 | 7.0s\n",
      "step   12700 | loss 1.6711 | lr 1.06e-04 | 7.1s\n",
      "step   12750 | loss 1.8130 | lr 1.05e-04 | 7.1s\n",
      "step   12800 | loss 1.8925 | lr 1.04e-04 | 7.1s\n",
      "step   12850 | loss 1.8911 | lr 1.02e-04 | 7.1s\n",
      "step   12900 | loss 1.7733 | lr 1.01e-04 | 7.1s\n",
      "step   12950 | loss 2.0157 | lr 9.99e-05 | 7.0s\n",
      "step   13000 | loss 1.7833 | lr 9.87e-05 | 7.1s\n",
      "  eval | val_loss 5.7491 | ppl 313.90\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step13000.pt\n",
      "step   13050 | loss 1.5013 | lr 9.75e-05 | 12.1s\n",
      "step   13100 | loss 1.5453 | lr 9.62e-05 | 7.0s\n",
      "step   13150 | loss 1.6261 | lr 9.50e-05 | 7.0s\n",
      "step   13200 | loss 1.5481 | lr 9.38e-05 | 7.1s\n",
      "step   13250 | loss 1.7404 | lr 9.26e-05 | 7.1s\n",
      "step   13300 | loss 1.6948 | lr 9.14e-05 | 7.1s\n",
      "step   13350 | loss 1.8291 | lr 9.02e-05 | 7.1s\n",
      "step   13400 | loss 1.6961 | lr 8.90e-05 | 7.1s\n",
      "step   13450 | loss 1.7035 | lr 8.78e-05 | 7.0s\n",
      "step   13500 | loss 1.7293 | lr 8.66e-05 | 7.1s\n",
      "  eval | val_loss 5.8368 | ppl 342.69\n",
      "step   13550 | loss 1.8204 | lr 8.54e-05 | 10.1s\n",
      "step   13600 | loss 1.4854 | lr 8.42e-05 | 7.1s\n",
      "step   13650 | loss 1.4811 | lr 8.31e-05 | 7.0s\n",
      "step   13700 | loss 1.5890 | lr 8.19e-05 | 7.1s\n",
      "step   13750 | loss 1.6169 | lr 8.07e-05 | 7.1s\n",
      "step   13800 | loss 1.5962 | lr 7.96e-05 | 7.1s\n",
      "step   13850 | loss 1.4638 | lr 7.84e-05 | 7.0s\n",
      "step   13900 | loss 1.5844 | lr 7.73e-05 | 7.1s\n",
      "step   13950 | loss 1.6316 | lr 7.61e-05 | 7.0s\n",
      "step   14000 | loss 1.4915 | lr 7.50e-05 | 7.1s\n",
      "  eval | val_loss 5.8926 | ppl 362.34\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step14000.pt\n",
      "step   14050 | loss 1.6014 | lr 7.39e-05 | 12.1s\n",
      "step   14100 | loss 1.6315 | lr 7.27e-05 | 7.1s\n",
      "step   14150 | loss 1.6937 | lr 7.16e-05 | 7.0s\n",
      "step   14200 | loss 1.6252 | lr 7.05e-05 | 7.1s\n",
      "step   14250 | loss 1.4368 | lr 6.94e-05 | 7.1s\n",
      "step   14300 | loss 1.4383 | lr 6.83e-05 | 7.1s\n",
      "step   14350 | loss 1.3736 | lr 6.72e-05 | 7.0s\n",
      "step   14400 | loss 1.5355 | lr 6.61e-05 | 7.1s\n",
      "step   14450 | loss 1.3826 | lr 6.50e-05 | 7.0s\n",
      "step   14500 | loss 1.5919 | lr 6.40e-05 | 7.1s\n",
      "  eval | val_loss 5.9627 | ppl 388.66\n",
      "step   14550 | loss 1.4638 | lr 6.29e-05 | 10.1s\n",
      "step   14600 | loss 1.3888 | lr 6.18e-05 | 7.1s\n",
      "step   14650 | loss 1.5348 | lr 6.08e-05 | 7.0s\n",
      "step   14700 | loss 1.6017 | lr 5.97e-05 | 7.1s\n",
      "step   14750 | loss 1.4714 | lr 5.87e-05 | 7.1s\n",
      "step   14800 | loss 1.4087 | lr 5.77e-05 | 7.1s\n",
      "step   14850 | loss 1.3949 | lr 5.66e-05 | 7.1s\n",
      "step   14900 | loss 1.5290 | lr 5.56e-05 | 7.1s\n",
      "step   14950 | loss 1.5194 | lr 5.46e-05 | 7.1s\n",
      "step   15000 | loss 1.3599 | lr 5.36e-05 | 7.1s\n",
      "  eval | val_loss 6.0080 | ppl 406.68\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step15000.pt\n",
      "step   15050 | loss 1.3480 | lr 5.26e-05 | 12.2s\n",
      "step   15100 | loss 1.3282 | lr 5.16e-05 | 7.1s\n",
      "step   15150 | loss 1.4305 | lr 5.06e-05 | 7.0s\n",
      "step   15200 | loss 1.5134 | lr 4.96e-05 | 7.0s\n",
      "step   15250 | loss 1.4604 | lr 4.87e-05 | 7.0s\n",
      "step   15300 | loss 1.4939 | lr 4.77e-05 | 7.1s\n",
      "step   15350 | loss 1.4100 | lr 4.67e-05 | 7.0s\n",
      "step   15400 | loss 1.2900 | lr 4.58e-05 | 7.1s\n",
      "step   15450 | loss 1.2066 | lr 4.49e-05 | 7.1s\n",
      "step   15500 | loss 1.4478 | lr 4.39e-05 | 7.1s\n",
      "  eval | val_loss 6.0505 | ppl 424.30\n",
      "step   15550 | loss 1.4988 | lr 4.30e-05 | 10.0s\n",
      "step   15600 | loss 1.4176 | lr 4.21e-05 | 7.1s\n",
      "step   15650 | loss 1.3133 | lr 4.12e-05 | 7.0s\n",
      "step   15700 | loss 1.3374 | lr 4.03e-05 | 7.1s\n",
      "step   15750 | loss 1.3782 | lr 3.94e-05 | 7.1s\n",
      "step   15800 | loss 1.4467 | lr 3.85e-05 | 7.1s\n",
      "step   15850 | loss 1.3033 | lr 3.77e-05 | 7.1s\n",
      "step   15900 | loss 1.3924 | lr 3.68e-05 | 7.0s\n",
      "step   15950 | loss 1.3458 | lr 3.59e-05 | 7.0s\n",
      "step   16000 | loss 1.2762 | lr 3.51e-05 | 7.1s\n",
      "  eval | val_loss 6.0823 | ppl 438.02\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step16000.pt\n",
      "step   16050 | loss 1.3856 | lr 3.43e-05 | 12.1s\n",
      "step   16100 | loss 1.2602 | lr 3.34e-05 | 7.1s\n",
      "step   16150 | loss 1.2407 | lr 3.26e-05 | 7.0s\n",
      "step   16200 | loss 1.2823 | lr 3.18e-05 | 7.1s\n",
      "step   16250 | loss 1.2417 | lr 3.10e-05 | 7.0s\n",
      "step   16300 | loss 1.2686 | lr 3.02e-05 | 7.1s\n",
      "step   16350 | loss 1.2000 | lr 2.94e-05 | 7.1s\n",
      "step   16400 | loss 1.4702 | lr 2.86e-05 | 7.1s\n",
      "step   16450 | loss 1.2860 | lr 2.79e-05 | 7.0s\n",
      "step   16500 | loss 1.2736 | lr 2.71e-05 | 7.1s\n",
      "  eval | val_loss 6.1148 | ppl 452.49\n",
      "step   16550 | loss 1.2139 | lr 2.64e-05 | 10.1s\n",
      "step   16600 | loss 1.2941 | lr 2.56e-05 | 7.1s\n",
      "step   16650 | loss 1.3414 | lr 2.49e-05 | 7.0s\n",
      "step   16700 | loss 1.3082 | lr 2.42e-05 | 7.1s\n",
      "step   16750 | loss 1.1933 | lr 2.35e-05 | 7.0s\n",
      "step   16800 | loss 1.3017 | lr 2.28e-05 | 7.1s\n",
      "step   16850 | loss 1.2013 | lr 2.21e-05 | 7.0s\n",
      "step   16900 | loss 1.3983 | lr 2.14e-05 | 7.1s\n",
      "step   16950 | loss 1.2635 | lr 2.08e-05 | 7.1s\n",
      "step   17000 | loss 1.3094 | lr 2.01e-05 | 7.1s\n",
      "  eval | val_loss 6.1409 | ppl 464.46\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step17000.pt\n",
      "step   17050 | loss 1.1683 | lr 1.94e-05 | 12.0s\n",
      "step   17100 | loss 1.2965 | lr 1.88e-05 | 7.1s\n",
      "step   17150 | loss 1.2066 | lr 1.82e-05 | 7.0s\n",
      "step   17200 | loss 1.2576 | lr 1.76e-05 | 7.1s\n",
      "step   17250 | loss 1.2655 | lr 1.69e-05 | 7.0s\n",
      "step   17300 | loss 1.2505 | lr 1.63e-05 | 7.1s\n",
      "step   17350 | loss 1.0917 | lr 1.58e-05 | 7.1s\n",
      "step   17400 | loss 1.2240 | lr 1.52e-05 | 7.1s\n",
      "step   17450 | loss 1.2653 | lr 1.46e-05 | 7.0s\n",
      "step   17500 | loss 1.3636 | lr 1.41e-05 | 7.1s\n",
      "  eval | val_loss 6.1644 | ppl 475.52\n",
      "step   17550 | loss 1.2095 | lr 1.35e-05 | 10.1s\n",
      "step   17600 | loss 1.2146 | lr 1.30e-05 | 7.1s\n",
      "step   17650 | loss 1.1729 | lr 1.24e-05 | 7.0s\n",
      "step   17700 | loss 1.2209 | lr 1.19e-05 | 7.1s\n",
      "step   17750 | loss 1.2049 | lr 1.14e-05 | 7.1s\n",
      "step   17800 | loss 1.1448 | lr 1.09e-05 | 7.1s\n",
      "step   17850 | loss 1.3294 | lr 1.04e-05 | 7.1s\n",
      "step   17900 | loss 1.2695 | lr 9.96e-06 | 7.1s\n",
      "step   17950 | loss 1.2027 | lr 9.50e-06 | 7.0s\n",
      "step   18000 | loss 1.2158 | lr 9.05e-06 | 7.1s\n",
      "  eval | val_loss 6.1771 | ppl 481.61\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step18000.pt\n",
      "step   18050 | loss 1.2036 | lr 8.60e-06 | 12.0s\n",
      "step   18100 | loss 1.1891 | lr 8.17e-06 | 7.1s\n",
      "step   18150 | loss 1.2157 | lr 7.75e-06 | 7.0s\n",
      "step   18200 | loss 1.2138 | lr 7.34e-06 | 7.1s\n",
      "step   18250 | loss 1.2216 | lr 6.94e-06 | 7.1s\n",
      "step   18300 | loss 1.3523 | lr 6.55e-06 | 7.1s\n",
      "step   18350 | loss 1.2246 | lr 6.18e-06 | 7.1s\n",
      "step   18400 | loss 1.1922 | lr 5.81e-06 | 7.1s\n",
      "step   18450 | loss 1.2077 | lr 5.46e-06 | 7.0s\n",
      "step   18500 | loss 1.1549 | lr 5.11e-06 | 7.1s\n",
      "  eval | val_loss 6.1852 | ppl 485.50\n",
      "step   18550 | loss 1.1617 | lr 4.78e-06 | 10.1s\n",
      "step   18600 | loss 1.2086 | lr 4.46e-06 | 7.1s\n",
      "step   18650 | loss 1.1548 | lr 4.14e-06 | 7.1s\n",
      "step   18700 | loss 1.2689 | lr 3.84e-06 | 7.1s\n",
      "step   18750 | loss 1.2623 | lr 3.56e-06 | 7.0s\n",
      "step   18800 | loss 1.1811 | lr 3.28e-06 | 7.1s\n",
      "step   18850 | loss 1.2051 | lr 3.01e-06 | 7.0s\n",
      "step   18900 | loss 1.2485 | lr 2.76e-06 | 7.1s\n",
      "step   18950 | loss 1.1641 | lr 2.51e-06 | 7.0s\n",
      "step   19000 | loss 1.2357 | lr 2.28e-06 | 7.1s\n",
      "  eval | val_loss 6.1889 | ppl 487.31\n",
      "  ✅ saved: C:\\workspace\\GPT2\\models\\ckpt_step19000.pt\n",
      "step   19050 | loss 1.2017 | lr 2.06e-06 | 12.0s\n",
      "step   19100 | loss 1.2375 | lr 1.85e-06 | 7.1s\n",
      "step   19150 | loss 1.1782 | lr 1.65e-06 | 7.0s\n",
      "step   19200 | loss 1.1528 | lr 1.46e-06 | 7.1s\n",
      "step   19250 | loss 1.2746 | lr 1.28e-06 | 7.0s\n",
      "step   19300 | loss 1.0978 | lr 1.12e-06 | 7.1s\n",
      "step   19350 | loss 1.1557 | lr 9.64e-07 | 7.0s\n",
      "step   19400 | loss 1.1945 | lr 8.22e-07 | 7.1s\n",
      "step   19450 | loss 1.1853 | lr 6.91e-07 | 7.1s\n",
      "step   19500 | loss 1.1266 | lr 5.71e-07 | 7.1s\n",
      "  eval | val_loss 6.1907 | ppl 488.20\n",
      "step   19550 | loss 1.1712 | lr 4.62e-07 | 10.1s\n",
      "step   19600 | loss 1.3459 | lr 3.65e-07 | 7.1s\n",
      "step   19650 | loss 1.1612 | lr 2.80e-07 | 7.0s\n",
      "step   19700 | loss 1.1687 | lr 2.06e-07 | 7.1s\n",
      "step   19750 | loss 1.1394 | lr 1.43e-07 | 7.0s\n",
      "step   19800 | loss 1.2175 | lr 9.14e-08 | 7.1s\n",
      "step   19850 | loss 1.2134 | lr 5.14e-08 | 7.0s\n",
      "step   19900 | loss 1.1140 | lr 2.28e-08 | 7.1s\n",
      "step   19950 | loss 1.2886 | lr 5.71e-09 | 7.1s\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
