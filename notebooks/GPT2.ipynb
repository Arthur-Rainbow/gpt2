{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-28T15:26:23.887474Z",
     "start_time": "2026-01-28T15:26:23.884193Z"
    }
   },
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from asttokens.util import stmt_class_names\n",
    "from jinja2.compiler import optimizeconst\n",
    "from networkx.algorithms.mis import maximal_independent_set\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ],
   "id": "b2969c30f8d066af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class GPT2Config:\n",
    "    vocab_size: int = 50257\n",
    "    block_size: int = 256\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True\n"
   ],
   "id": "d18809501af1f0c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, n_embd, biais=True, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(n_embd))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_embd)) if biais else None\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, (x.size(-1),), self.weight, self.bias, self.eps)"
   ],
   "id": "ddfc917efce5ea48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:50:36.384813Z",
     "start_time": "2026-01-28T14:50:36.378528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(config.dropout)\n",
    "        self.resid_drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        mask = torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size,\n",
    "                                                                                 config.block_size)\n",
    "        self.register_buffer(\"biais\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.biais[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.c_proj(y)\n",
    "        y = self.resid_drop(y)\n",
    "\n",
    "        return y\n"
   ],
   "id": "8f426c71ae918711",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:17:00.372139Z",
     "start_time": "2026-01-28T15:17:00.367750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ],
   "id": "ed84250e84c325b5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:17:00.536661Z",
     "start_time": "2026-01-28T15:17:00.533274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, biais=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, biais=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ],
   "id": "65ca5d089de0cd2f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:17:00.869905Z",
     "start_time": "2026-01-28T15:17:00.864765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, biais=config.bias)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size\n",
    "\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.wte(idx) + self.wpe(pos)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss"
   ],
   "id": "c01e0ecf2da6b390",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:17:01.556573Z",
     "start_time": "2026-01-28T15:17:01.352474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = GPT2Config(block_size=128, n_layer=4, n_head=4, n_embd=256, dropout=0.1)\n",
    "model = GPT2(config).to(device)\n",
    "\n",
    "x = torch.randint(0, config.vocab_size, (2, 32), device=device)\n",
    "logits, loss = model(x, x)\n",
    "logits.shape, loss.item()"
   ],
   "id": "fa143811d5139876",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 32, 50257]), 10.530092239379883)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:19:18.670408Z",
     "start_time": "2026-01-28T15:19:13.704745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "ds"
   ],
   "id": "5c46ede4bc47e0e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\GPT2\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\libry\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 398673.40 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 2040685.51 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 536542.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:20:08.750251Z",
     "start_time": "2026-01-28T15:20:05.596236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # pratique pour batcher"
   ],
   "id": "7196d7c63964f6d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\GPT2\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\libry\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:20:40.205951Z",
     "start_time": "2026-01-28T15:20:38.292973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "\n",
    "tokenized = ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized"
   ],
   "id": "c6d279f4816f0580",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4358/4358 [00:00<00:00, 18564.41 examples/s]\n",
      "Map: 100%|██████████| 36718/36718 [00:00<00:00, 37471.55 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:00<00:00, 37628.47 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:20:55.264272Z",
     "start_time": "2026-01-28T15:20:53.750749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# concat tous les input_ids en une grande liste\n",
    "def build_stream(split, key=\"input_ids\"):\n",
    "    ids = []\n",
    "    for ex in split:\n",
    "        ids.extend(ex[key])\n",
    "        ids.append(tokenizer.eos_token_id)\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_stream = build_stream(tokenized[\"train\"])\n",
    "val_stream = build_stream(tokenized[\"validation\"])\n",
    "\n",
    "len(train_stream), len(val_stream)"
   ],
   "id": "3b3100051707028e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2428602, 251049)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:27:53.516601Z",
     "start_time": "2026-01-28T15:27:53.508372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StreamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, stream_ids: torch.Tensor, block_size: int):\n",
    "        self.data = stream_ids\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - 1) // self.block_size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.block_size\n",
    "        x = self.data[start: start + self.block_size]\n",
    "        y = self.data[start + 1: start + 1 + self.block_size]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_ds = StreamDataset(train_stream, config.block_size)\n",
    "val_ds = StreamDataset(val_stream, config.block_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, drop_last=True)\n",
    "\n",
    "next(iter(train_loader))[0].shape"
   ],
   "id": "6282a06a373a1c5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:30:22.176444Z",
     "start_time": "2026-01-28T15:30:22.173531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_lr(step, warmup_steps, max_steps, max_lr):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * step / warmup_steps\n",
    "\n",
    "    progress = (step - warmup_steps) / max(1, (max_steps - warmup_steps))\n",
    "    return 0.5 * max_lr * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.01)"
   ],
   "id": "800b99a50f052f36",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:31:51.211429Z",
     "start_time": "2026-01-28T15:31:51.207061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=50):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)"
   ],
   "id": "e3bd6ac58bbeca7a",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:38:33.829268Z",
     "start_time": "2026-01-28T15:37:57.371652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "use_amp = device == \"cuda\"\n",
    "scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "max_steps = 2000\n",
    "warmup_steps = 200\n",
    "max_lr = 3e-4\n",
    "grad_clip = 1.0\n",
    "log_every = 50\n",
    "eval_every = 200\n",
    "\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "step = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for x, y in train_loader:\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "\n",
    "        lr = get_lr(step, warmup_steps, max_steps, max_lr)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] = lr\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=use_amp):\n",
    "            _, loss = model(x, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            dt = time.time() - t0\n",
    "            print(f\"step {step:5d} | loss {loss.item():.4f} | lr {lr:.2e} | dt {dt:.1f}\")\n",
    "            t0 = time.time()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    if step >= max_steps:\n",
    "        break"
   ],
   "id": "7273926505d4797f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\libry\\AppData\\Local\\Temp\\ipykernel_19360\\2382721974.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "C:\\Users\\libry\\AppData\\Local\\Temp\\ipykernel_19360\\2382721974.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0 | loss 10.8745 | lr 0.00e+00 | dt 0.4\n",
      "step    50 | loss 10.0340 | lr 7.50e-05 | dt 0.9\n",
      "step   100 | loss 8.5851 | lr 1.50e-04 | dt 0.9\n",
      "step   150 | loss 7.4156 | lr 2.25e-04 | dt 0.9\n",
      "step   200 | loss 7.0035 | lr 3.00e-04 | dt 0.9\n",
      "step   250 | loss 6.8241 | lr 2.99e-04 | dt 0.9\n",
      "step   300 | loss 6.5002 | lr 2.98e-04 | dt 0.9\n",
      "step   350 | loss 6.6637 | lr 2.95e-04 | dt 0.9\n",
      "step   400 | loss 6.2998 | lr 2.91e-04 | dt 0.9\n",
      "step   450 | loss 6.6390 | lr 2.86e-04 | dt 0.9\n",
      "step   500 | loss 6.2773 | lr 2.80e-04 | dt 0.9\n",
      "step   550 | loss 6.4866 | lr 2.73e-04 | dt 0.9\n",
      "step   600 | loss 6.1682 | lr 2.65e-04 | dt 0.9\n",
      "step   650 | loss 6.4175 | lr 2.56e-04 | dt 1.0\n",
      "step   700 | loss 6.2013 | lr 2.46e-04 | dt 1.0\n",
      "step   750 | loss 6.3738 | lr 2.36e-04 | dt 0.9\n",
      "step   800 | loss 6.2861 | lr 2.25e-04 | dt 0.9\n",
      "step   850 | loss 6.1527 | lr 2.13e-04 | dt 0.9\n",
      "step   900 | loss 6.0703 | lr 2.01e-04 | dt 0.9\n",
      "step   950 | loss 6.1933 | lr 1.89e-04 | dt 0.9\n",
      "step  1000 | loss 6.0555 | lr 1.76e-04 | dt 0.9\n",
      "step  1050 | loss 6.0449 | lr 1.63e-04 | dt 0.9\n",
      "step  1100 | loss 6.1001 | lr 1.50e-04 | dt 0.9\n",
      "step  1150 | loss 5.8730 | lr 1.37e-04 | dt 0.9\n",
      "step  1200 | loss 5.9324 | lr 1.24e-04 | dt 0.9\n",
      "step  1250 | loss 5.5398 | lr 1.11e-04 | dt 0.9\n",
      "step  1300 | loss 5.8750 | lr 9.87e-05 | dt 0.9\n",
      "step  1350 | loss 5.9214 | lr 8.66e-05 | dt 0.9\n",
      "step  1400 | loss 5.6469 | lr 7.50e-05 | dt 0.9\n",
      "step  1450 | loss 5.8885 | lr 6.40e-05 | dt 0.9\n",
      "step  1500 | loss 5.9248 | lr 5.36e-05 | dt 0.9\n",
      "step  1550 | loss 5.9608 | lr 4.39e-05 | dt 0.9\n",
      "step  1600 | loss 5.7431 | lr 3.51e-05 | dt 0.9\n",
      "step  1650 | loss 5.8594 | lr 2.71e-05 | dt 0.9\n",
      "step  1700 | loss 5.8405 | lr 2.01e-05 | dt 0.9\n",
      "step  1750 | loss 5.7817 | lr 1.41e-05 | dt 0.9\n",
      "step  1800 | loss 5.8381 | lr 9.05e-06 | dt 0.9\n",
      "step  1850 | loss 6.0572 | lr 5.11e-06 | dt 0.9\n",
      "step  1900 | loss 5.8622 | lr 2.28e-06 | dt 0.9\n",
      "step  1950 | loss 5.9750 | lr 5.71e-07 | dt 0.9\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:46:46.537020Z",
     "start_time": "2026-01-28T15:46:46.533611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt, max_new_tokens=80, temperature=1.0, top_k=0):\n",
    "    model.eval()\n",
    "    idx = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.config.block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, ix = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = float(\"-inf\")\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    out = tokenizer.decode(idx[0].tolist())\n",
    "    model.train()\n",
    "    return out\n"
   ],
   "id": "d587e78b4e1936cb",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:47:20.667524Z",
     "start_time": "2026-01-28T15:47:20.392807Z"
    }
   },
   "cell_type": "code",
   "source": "print(generate(model, \"In the beginning\", max_new_tokens=80, temperature=0.9, top_k=40))",
   "id": "1ca88efa987ad8d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning of the song 's death of the song was the song \" . He was \" for the play of a episode , \" ( \" ) \" ) \" , which became \" the \" \" and \" an \" which was the song \" . \n",
      "<|endoftext|> Other songs , \" , he is about \" \" The first released the video \" , \" the song \" for \" , \" . \" and \"\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:49:55.171343Z",
     "start_time": "2026-01-28T15:49:55.091676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "save_dir = r\"C:\\workspace\\GPT2\\models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "ckpt_path = os.path.join(save_dir, \"gpt2_scratch.pt\")\n",
    "ckpt_path\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"config\": config.__dict__,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "    },\n",
    "    ckpt_path\n",
    ")\n",
    "\n",
    "os.path.exists(ckpt_path)\n",
    "print(f\"✅ Modèle sauvegardé dans : {ckpt_path}\")"
   ],
   "id": "87098a91ebf3441b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle sauvegardé dans : C:\\workspace\\GPT2\\models\\gpt2_scratch.pt\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:50:01.819649Z",
     "start_time": "2026-01-28T15:50:01.618806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "config = GPT2Config(**ckpt[\"config\"])\n",
    "model = GPT2(config).to(device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()"
   ],
   "id": "a2f2707fd539216f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (wte): Embedding(50257, 256)\n",
       "  (wpe): Embedding(128, 256)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-3): 4 x Block(\n",
       "      (ln_1): LayerNorm()\n",
       "      (attn): CausalSelfAttention(\n",
       "        (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2392287d2a3425ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
